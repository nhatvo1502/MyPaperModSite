[{"content":" Intro: Recently, I explored a simple yet powerful way to achieve image processing using AWS Lambda and AWS Rekognition. With Machine Learning on the rise, I think it would be a cool project for me to get my hands on this topic. In this post, I’ll walk through a scalable and cost-effective approach: using AWS Lambda to process images stored in an S3 bucket, analyze their content with Rekognition to identify objects, faces, or text, and then store the extracted metadata in another S3 bucket for further use.\nTerraform: 1. S3 I created two S3 buckets: one for input images and one for storing Rekognition metadata and resized images. While it is possible to use a single bucket, I would not recommend it, as putting a new image into the same bucket that triggers the Lambda function could cause the application to enter an infinite loop.\n# S3 resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;input_bucket\u0026#34; { bucket = \u0026#34;serverless-image-input-bucket\u0026#34; } resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;output_bucket\u0026#34; { bucket = \u0026#34;serverless-image-out-bucket\u0026#34; } 2. Lambda I created a Lambda function using Python 3.9 and used a customized layer from the user \u0026ldquo;Klayers\u0026rdquo; to import the PIL library.\n# Lambda Function resource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;image_processor\u0026#34; { filename = \u0026#34;lambda_function.zip\u0026#34; function_name = \u0026#34;image-processing-lambda\u0026#34; role = aws_iam_role.lambda_role.arn handler = \u0026#34;lambda_function.lambda_handler\u0026#34; runtime = \u0026#34;python3.9\u0026#34; timeout = 10 source_code_hash = filebase64sha256(\u0026#34;lambda_function.zip\u0026#34;) layers = [\u0026#34;arn:aws:lambda:us-east-1:770693421928:layer:Klayers-p39-pillow:1\u0026#34;] environment { variables = { OUTPUT_BUCKET = aws_s3_bucket.output_bucket.id } } } resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;allow_s3\u0026#34; { statement_id = \u0026#34;AllowS3Invoke\u0026#34; action = \u0026#34;lambda:InvokeFunction\u0026#34; principal = \u0026#34;s3.amazonaws.com\u0026#34; function_name = aws_lambda_function.image_processor.function_name source_arn = aws_s3_bucket.input_bucket.arn } 3. Permissions and Roles I created a custom inline IAM role for the Lambda function and granted it the necessary permissions to execute tasks involving both S3 buckets and the CloudWatch LogGroup.\nresource \u0026#34;aws_iam_role\u0026#34; \u0026#34;lambda_role\u0026#34; { name = \u0026#34;lambda-image-processing-role\u0026#34; assume_role_policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [{ Action = \u0026#34;sts:AssumeRole\u0026#34; Effect = \u0026#34;Allow\u0026#34; Principal = { Service = \u0026#34;lambda.amazonaws.com\u0026#34; } }] }) } resource \u0026#34;aws_iam_policy_attachment\u0026#34; \u0026#34;lambda_policy\u0026#34; { name = \u0026#34;lambda-s3-rekognition-policy\u0026#34; roles = [aws_iam_role.lambda_role.name] policy_arn = \u0026#34;arn:aws:iam::aws:policy/AmazonS3FullAccess\u0026#34; } resource \u0026#34;aws_iam_policy_attachment\u0026#34; \u0026#34;lambda_rekognition_policy\u0026#34; { name = \u0026#34;lambda-rekognition-policy\u0026#34; roles = [aws_iam_role.lambda_role.name] policy_arn = \u0026#34;arn:aws:iam::aws:policy/AmazonRekognitionFullAccess\u0026#34; } resource \u0026#34;aws_iam_role_policy\u0026#34; \u0026#34;logging\u0026#34; { name = \u0026#34;lambda-image-processing-logging-policy\u0026#34; role = aws_iam_role.lambda_role.name policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34;, Statement = [{ Effect = \u0026#34;Allow\u0026#34;, Action = [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], Resource = [\u0026#34;arn:aws:logs:*:566027688242:log-group:*:log-stream:*\u0026#34;] }] }) } 4. S3 Triggers Finally, I created a trigger event so that every time a new object is added to the input S3 bucket, it will invoke the Lambda function to process the image. The depends_on attribute ensures that the Lambda function is created before the trigger is assigned.\nresource \u0026#34;aws_s3_bucket_notification\u0026#34; \u0026#34;s3_trigger\u0026#34; { bucket = aws_s3_bucket.input_bucket.id lambda_function { lambda_function_arn = aws_lambda_function.image_processor.arn events = [\u0026#34;s3:ObjectCreated:*\u0026#34;] } depends_on = [aws_lambda_function.image_processor] # Ensure the lambda function is created first } Deploy: 1. AWS CLI I created an AWS CLI account with full permissions since I will deploy the Terraform files remotely from my local computer.\n2. Terraform CLI: terraform fmt # Format main.tf to ensure consistency in syntax terraform validate # Validate the configuration terraform plan # Review what I\u0026#39;m about to deploy terraform apply --auto-approve # Deploy main.tf Testing: The test is straightforward. I upload a picture of a random animal into the input S3 bucket. The presence of a new object in this bucket triggers the Lambda function, which also logs an entry in the CloudWatch LogGroup. The Lambda function performs two tasks:\nMakes a copy of the original picture, resizes it, and stores it in the output bucket. Uses AWS Rekognition to analyze the image content and outputs the metadata as a JSON file into the output bucket. But first thing first, I will check if my terraform file has been deployed correctly by running this command:\nterraform state list It works perfectly, and here is the result. Refinement: I have only scratched the surface of AWS Rekognition\u0026rsquo;s use cases. This small application can be scaled up to handle a large volume of images in bulk where Rekognition has a lot more to offer.\nResources and Credits Klayer https://github.com/keithrozario/Klayers Rekognition Python https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/ex-lambda.html\nCheck out this project on my GitHub.\n","permalink":"http://www.nvo.one/posts/rekcognition/","summary":"Intro: Recently, I explored a simple yet powerful way to achieve image processing using AWS Lambda and AWS Rekognition. With Machine Learning on the rise, I think it would be a cool project for me to get my hands on this topic. In this post, I’ll walk through a scalable and cost-effective approach: using AWS Lambda to process images stored in an S3 bucket, analyze their content with Rekognition to identify objects, faces, or text, and then store the extracted metadata in another S3 bucket for further use.","title":"Rekognition"},{"content":"\rCheck out my project on GitHub\nThe Goal: The idea is to use Infrastructure as Code (IaC) to quickly spin up a simple yet secure virtual network on AWS. Terraform CLI is my tool of choice for managing it all, so I can automate the setup and ensure everything stays organized and reproducible.\nWhat’s Inside the Infrastructure: A VPC in the us-west-1 region with a CIDR block of 10.0.0.0/16—this is the foundation of the network. A Public Subnet (10.0.1.0/24) connected to the internet via an Internet Gateway. The cool thing is that all the traffic from this subnet can flow freely to the internet, thanks to a simple Public Route Table. A Private Subnet (10.0.2.0/24) that’s a little more closed off—this one doesn’t have direct internet access. A NAT Gateway placed in the public subnet, with an Elastic IP. This allows instances in the private subnet to reach the internet for things like updates, but without exposing them to the outside world. We handle all this securely with a Private Route Table. Testing: I launched a t3.micro Linux instance in both the Public Subnet and Private Subnet. First, I SSH into the Public Instance using its public IP address. Then, I recreated my private key and stored it on the Public Instance, before SSH into the Private Instance using its private IP address. From Private Instance, I couldn\u0026rsquo;t ping google.com nor able to yum update it. The first thing I check was my Route Tables and voila, looks like I forgot to associate my Private RT with Private subnet, therefore AWS has to created a default RT and associated it with the Private Subnet which is not what we want because our instance cannot reach NAT Gateway therefore no connection. I went back to my main.tf and added this block\u0026hellip; # Associate Private Route Table with Private Subnet\rresource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;private\u0026#34; {\rsubnet_id = aws_subnet.terraform_private_subnet_1.id\rroute_table_id = aws_route_table.terraform_rt_private_1.id\r} \u0026hellip;rerun terraform\u0026hellip;\nterraform plan\rterraform apply --auto-approve \u0026hellip;make sure Private Route Table is in placed, as you can see, the Private Subnet is now properly\u0026hellip; \u0026hellip; I can ping and update Private Instance as the traffic has succesfully flows through the Private Route Table and reach NAT Gateway! Key Pairs and Security Notes: To keep things simple, I generated a public/private key pair on my local computer using PowerShell: ssh-keygen -t rsa -b 2048 -f C:\\path\\to\\your\\keyfile I used this key pair for both the Public and Private Instances. To SSH into the private instance, I created a new private key on the Public Instance, pasted it in, and used it for access. I know this isn’t the most secure way to handle things, but it helped me keep it simple for this demo. Room for Improvement: Going forward, it would be much better to use AWS Secrets Manager to securely store and manage secrets. This would help scale things more securely and take the guesswork out of key management. ","permalink":"http://www.nvo.one/posts/my-first-terraform/","summary":"Check out my project on GitHub\nThe Goal: The idea is to use Infrastructure as Code (IaC) to quickly spin up a simple yet secure virtual network on AWS. Terraform CLI is my tool of choice for managing it all, so I can automate the setup and ensure everything stays organized and reproducible.\nWhat’s Inside the Infrastructure: A VPC in the us-west-1 region with a CIDR block of 10.0.0.0/16—this is the foundation of the network.","title":"My First Terraform"},{"content":"\rHey there! Welcome to my tech blog. I’m excited to share a little project: automating Shopify orders data into a database warehouse and setting up the connection for analysis. I wanted a simple way to fetch data and get real-time insights without all the manual effort. So, I set up AWS Event Scheduler to trigger data retrieval, used AWS Lambda for serverless processing, and chose DynamoDB for storage. To connect DynamoDB to Microsoft Power BI, I used CDATA Universal Software to create an ODBC driver.\nI\u0026rsquo;m sure there are more sophisticated solutions out there and this is my humble approach, but I hope you find this useful! Let’s turn Shopify orders into awesome business insights together! Setup AWS Lambda: I created a Lambda function to fetch data from Shopify and store it in DynamoDB. AWS Event Scheduler: I use Event Scheduler to trigger the Lambda function daily. AWS DynamoDB: While a relational database might be better for a data warehouse, I opted for DynamoDB for its simplicity and cost-effectiveness. CDATA Amazon DynamoDB ODBC Driver: I configured my DynamoDB ODBC driver with CDATA since they offer free trials. This driver is essential for pulling data into Power BI since it doesn’t support DynamoDB out-of-the-box. Power BI: Finally, I set up the ODBC driver as a data source in Power BI to pull in my data and start visualizing! CloudWatch: I also set up CloudWatch for logging to keep track of everything. Improvements Explore a Relational Database: Consider using Amazon RDS instead of DynamoDB for enhanced data management and querying capabilities, leading to more efficient analysis. Improve Error Handling: Implement more robust error handling in the AWS Lambda function to ensure better resilience during data retrieval. Flexible Scheduling: Adjust the data retrieval schedule based on business needs, rather than sticking to a fixed daily trigger, to optimize resource usage. Data Validation and Transformation: Incorporate processes for data validation and transformation before loading data into DynamoDB to improve overall data quality. Enhance Power BI Dashboards: Add more interactive elements and deeper analytics in Power BI to provide valuable insights for decision-making. Check out this project on my GitHub.\nThank you for visiting!\n","permalink":"http://www.nvo.one/posts/shopify-daily-report/","summary":"Hey there! Welcome to my tech blog. I’m excited to share a little project: automating Shopify orders data into a database warehouse and setting up the connection for analysis. I wanted a simple way to fetch data and get real-time insights without all the manual effort. So, I set up AWS Event Scheduler to trigger data retrieval, used AWS Lambda for serverless processing, and chose DynamoDB for storage. To connect DynamoDB to Microsoft Power BI, I used CDATA Universal Software to create an ODBC driver.","title":"Fetching Shopify API using AWS Lambda and DynamoDB"},{"content":"\rWelcome to my tech blog! Today, I’m excited to take you on my cloud journey with a project which I was inspired from real-world challenge: integrating SMS notifications into a software that had numerous restrictions. I needed a solution to notify users whenever the database was updated. Instead of wrestling with the software’s limitations, I decided to create a microservice that could handle SMS notifications independently. This way, the software team can invoke an API to send SMS messages without worrying about the underlying code and integration complexities.\nSetup In this project, I use Twilio for sending text messages, combined with AWS Lambda, AWS API Gateway, and AWS Route53 for domain and API mapping. Here’s how I set it up:\nTwilio Setup: I bought a number from Twilio, loaded a few bucks into the account, and created a Lambda function to send SMS whenever the API is invoked using the Twilio phone number. API Gateway Configuration: I created a POST method API with a custom domain name and mapped it to my Lambda function. Route53 Configuration: I updated an A record on Route53 with my API Gateway\u0026rsquo;s ARN to handle the domain and API mapping seamlessly. Check out my project on GitHub\nImprovement There are few improvements that I would like to integrate in the future are:\nAuthentication: I could add API key authentication through custom headers or integrate Okta for more advanced identiy management. This will ensure authorized users can trigger the SMS functionality. Loggin and Monitoring: Using AWS CloudWatch would help me keep track of performance and quickly resolve any issues. Reliability: Implementing retru mechanisms and error handling strategies into my lambda function would make the service more reliable, ensuring messages are sent even during temporary failures. By the end of this project, I happy with this robust and scalable SMS notification service that can be easily integrated into any application. Let’s get started!\n","permalink":"http://www.nvo.one/posts/twilio-microservice/","summary":"Welcome to my tech blog! Today, I’m excited to take you on my cloud journey with a project which I was inspired from real-world challenge: integrating SMS notifications into a software that had numerous restrictions. I needed a solution to notify users whenever the database was updated. Instead of wrestling with the software’s limitations, I decided to create a microservice that could handle SMS notifications independently. This way, the software team can invoke an API to send SMS messages without worrying about the underlying code and integration complexities.","title":"Send SMS with Twilio using AWS Lambda and API Gateway"},{"content":"\u0026ldquo;Humility is not thinking less of yourself, it\u0026rsquo;s thinking of yourself less.\u0026rdquo;\nRick Warren ","permalink":"http://www.nvo.one/posts/about-me/","summary":"\u0026ldquo;Humility is not thinking less of yourself, it\u0026rsquo;s thinking of yourself less.\u0026rdquo;\nRick Warren ","title":"About Me"}]
[{"content":"Deploy infrastructure using Terraform ¹, AWS Lambda ², and GitHub Actions ³. During a conversation with a close friend in Australia, I was inspired by the idea of using a lightweight framework like Flask to build a cloud application. Thanks @terryduong for the inspiration and support for this project.\nCheck out my project on GitHub\nStage 0: Game plan I split this project into five stages.\nStage 1: Creating the app Stage 2: Host locally Stage 3: Hosting on AWS using Console Stage 4: Hosting on AWS using Terraform Stage 5: Automate the deployment using Terraform and Github Actions A Reference List at the end of this page includes all the documents I consulted throughout this project.\nStage 1: Creating Nnote Thanks to Tech With Tim for this details tutorial\nI used a Python virtual environment to track all dependencies via requirements.txt, facilitating the subsequent creation of a Docker image.\npython -m venv .\\venv ./env/scripts/activate pip freeze \u0026gt; requirements.txt After becoming familiar with Python Flask, I challenged myself to add some features to this app. A password reset function seemed useful.\nFirst, I added a new button to the nav-bar in base.html\n\u0026lt;!--website/templates/base.html--\u0026gt; \u0026lt;div class=\u0026#34;collapse navbar-collapse\u0026#34; id=\u0026#34;navbar\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;navbar-nav\u0026#34;\u0026gt; {% if user.is_authenticated %} \u0026lt;a class=\u0026#34;nav-item nav-link\u0026#34; id=\u0026#34;home\u0026#34; href=\u0026#34;/\u0026#34;\u0026gt;Home\u0026lt;/a\u0026gt; \u0026lt;a class=\u0026#34;nav-item nav-link\u0026#34; id=\u0026#34;resetPassword\u0026#34; href=\u0026#34;/reset-password\u0026#34; \u0026gt;Reset Password\u0026lt;/a \u0026gt; \u0026lt;a class=\u0026#34;nav-item nav-link\u0026#34; id=\u0026#34;logout\u0026#34; href=\u0026#34;/logout\u0026#34;\u0026gt;Logout\u0026lt;/a\u0026gt; {% else %} \u0026lt;a class=\u0026#34;nav-item nav-link\u0026#34; id=\u0026#34;login\u0026#34; href=\u0026#34;/login\u0026#34;\u0026gt;Login\u0026lt;/a\u0026gt; \u0026lt;a class=\u0026#34;nav-item nav-link\u0026#34; id=\u0026#34;signUp\u0026#34; href=\u0026#34;/sign-up\u0026#34;\u0026gt;Sign Up\u0026lt;/a\u0026gt; {% endif %} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; Then, I created a new page, reset-password.html\n\u0026lt;!--website/templates/reset-password.html--\u0026gt; {% extends \u0026#34;base.html\u0026#34; %} {% block title %}Reset Password{% endblock %} {% block content %} \u0026lt;form method=\u0026#34;POST\u0026#34;\u0026gt; \u0026lt;h3 align=\u0026#34;center\u0026#34;\u0026gt;Reset Password\u0026lt;/h3\u0026gt; \u0026lt;div class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;password1\u0026#34;\u0026gt;Current Password\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; class=\u0026#34;form-control\u0026#34; id=\u0026#34;current_password\u0026#34; name=\u0026#34;current_password\u0026#34; placeholder=\u0026#34;Enter current password\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;password2\u0026#34;\u0026gt;New Password\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; class=\u0026#34;form-control\u0026#34; id=\u0026#34;new_password1\u0026#34; name=\u0026#34;new_password1\u0026#34; placeholder=\u0026#34;Enter new password\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;password2\u0026#34;\u0026gt;New Password (Confirm)\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; class=\u0026#34;form-control\u0026#34; id=\u0026#34;new_password2\u0026#34; name=\u0026#34;new_password2\u0026#34; placeholder=\u0026#34;Re-enter new password\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;br /\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34; class=\u0026#34;btn btn-primary\u0026#34;\u0026gt;Submit\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; {% endblock %} Finally, I added a new route to auth.py to handle the logic.\n#website/auth.py @auth.route(\u0026#39;/reset-password\u0026#39;, methods=[\u0026#39;GET\u0026#39;, \u0026#39;POST\u0026#39;]) def reset_password(): if request.method == \u0026#39;POST\u0026#39;: current_password = request.form.get(\u0026#39;current_password\u0026#39;) new_password1 = request.form.get(\u0026#39;new_password1\u0026#39;) new_password2 = request.form.get(\u0026#39;new_password2\u0026#39;) if check_password_hash(current_user.password, current_password) == False: flash(\u0026#39;Incorrect password!\u0026#39;, category=\u0026#39;error\u0026#39;) else: if len(new_password1) \u0026lt; 7: flash(\u0026#39;Password must be at least 7 characters.\u0026#39;, category=\u0026#39;error\u0026#39;) elif new_password1!=new_password2: flash(\u0026#39;Password don\\\u0026#39;t match\u0026#39;, category=\u0026#39;error\u0026#39;) else: current_user.password = generate_password_hash(new_password1, method=\u0026#39;pbkdf2:sha256\u0026#39;) db.session.commit() flash(\u0026#39;Password reset succesfully! Please login again.\u0026#39;) logout_user() return redirect(url_for(\u0026#39;auth.login\u0026#39;)) return render_template(\u0026#39;reset_password.html\u0026#39;, user=current_user) Stage 2: Hosting Locally 2a. Workflows: Create DB Run the app with Python Integrate Gunicorn Build app as Docker MySQL Server SQLite is lightweight and good for small project but AWS doesn\u0026rsquo;t support this engine officially. I chose MySQL because it\u0026rsquo;s cost-effective, user friendly and supported by AWS [1].\nInstall MySQL and MySQL Workbench\nUsername: root Password: password Update Connection String and Database Authentication To have nnote-app interact with MySQL server, I used sqlalchemy\nInstall sqlalchemy package\npip install sqlalchemy_utils Modified __init__.py\n#website/__init__.py from sqlalchemy_utils import database_exists, create_database #------------------------------------------------------------------ db = SQLAlchemy() DB_USERNAME = \u0026#34;root\u0026#34; DB_PASSWORD = \u0026#34;password\u0026#34; DB_HOST = \u0026#34;host.docker.internal\u0026#34; # run from inside Docker DB_NAME = \u0026#34;nnote_database\u0026#34; #------------------------------------------------------------------ def create(): app = Flask(__name__) app.config[\u0026#39;SECRET_KEY\u0026#39;] = \u0026#39;mysecretkey\u0026#39; url = f\u0026#34;mysql+pymysql://{DB_USERNAME}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\u0026#34; app.config[\u0026#39;SQLALCHEMY_DATABASE_URI\u0026#39;] = url #------------------------------------------------------------------ def create_db(url, app): # Check if the database exists, if not, create it print(\u0026#39;create_database is running\u0026#39;) if not database_exists(url): create_database(url) # Use the app context to create all tables with app.app_context(): db.create_all() print(\u0026#39;Created Database!\u0026#39;) I will do a quick test to see if my nnote-app can interact with the DB\npython .\\main.py \u0026hellip; and voila, the nnote-app succesfully connect to MySQL Database on local and created 2 tables: user and note just as how I wanted. Now, all we need to to pack all the dependencies into requirements.txt and get ready to build a Docker image. But before that, I need Gunicorn\nGunicorn\nWhile Flask is great for development, its server is not suitable for concurrent traffics in production envrinonment. I choose Gunicorn [2] to handle Flask\u0026rsquo;s shortcoming.\nTo install\u0026hellip;\npip install gunicorn export dependencies\u0026hellip;\npip freeze \u0026gt; requirements.txt Docker Install Docker\nI create Dockerfile under .\\ using gunicorn as my WGSI\nFROM python:3.11 WORKDIR /app COPY . . RUN pip3 install -r requirements.txt EXPOSE 5000 CMD [\u0026#34;gunicorn\u0026#34;, \u0026#34;-b\u0026#34;, \u0026#34;0.0.0.0:5000\u0026#34;, \u0026#34;website:create_app()\u0026#34;] 2b. Testing: build and run Docker\ndocker build -t nnote-app . docker run -p 5000:5000 nnote-app Docker Image is built succesfully, since I exposed port 5000 from inside the Docker container to port 5000 outside of the container, the correct URL to my nnote-app should be localhost:5000 or http://127.0.0.1:5000 Stage 2 is completed! I\u0026rsquo;m excited for the next one \u0026ndash; AWS Cloud.\nStage 3: Hosting on AWS using Console 3a. Workflows: What resources that I need on AWS: a MySQL Server, ECR-\u0026gt;ECS-\u0026gt;Task Definitaion-\u0026gt;Service-\u0026gt;Task(point back to my container image on ECR)\nDatabase\nCreate a MySQL db by navigating to AWS Console -\u0026gt; Amazon RDS -\u0026gt; Select Create database Configure: Engine options = MySQL Templates = Free tier Master username = admin Master password = password Public access = Yes Click Create database Upon the database creation, I grab the endpoint from AWS RDS console and use MySQL Workbench test for connection and authentication. Update nnote-app with endpoint, DB_USERNAME=\u0026quot;admin\u0026quot; and DB_PASSWORD=\u0026quot;password\u0026quot; #website/__init__.py #------------------------------------------------------------------ db = SQLAlchemy() DB_USERNAME = \u0026#34;admin\u0026#34; DB_PASSWORD = \u0026#34;password\u0026#34; DB_HOST = \u0026#34;AWS-RDS-Endpoint\u0026#34; # run from inside Docker DB_NAME = \u0026#34;nnote_database\u0026#34; #------------------------------------------------------------------ def create(): app = Flask(__name__) app.config[\u0026#39;SECRET_KEY\u0026#39;] = \u0026#39;mysecretkey\u0026#39; url = f\u0026#34;mysql+pymysql://{DB_USERNAME}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\u0026#34; app.config[\u0026#39;SQLALCHEMY_DATABASE_URI\u0026#39;] = url ECR\nTo create a Elastic Container Repo [3], navigate to AWS Console -\u0026gt; Amazon Elastic Container Registry -\u0026gt; repository -\u0026gt; Create depository Configure: Repository name = nnote-ecr/app Image tag mutability = Mutable Encryption = AES-256 Create Click our repo nnote-ecr.app -\u0026gt; View push commands -\u0026gt; follow the instruction from AWS to push my local Docker image to AWS ECR I also grabbed the the Repo URI on the way out for later ECS Cluster\nNavigate to AWS Console -\u0026gt; Amazon Elastic Container Service -\u0026gt; Create cluster Configure: Cluster name = nnote-us-east-1-cluster Infrastructure = AWS FarGate Create ECS Task definition\nIn AWS Elastic Container Service Console -\u0026gt; Task Definition Configure - Infrastructure: Task definition family = nnote-td Launch Type = AWS FarGate Task execution role = ecsTaskExecutionRole Configure - Container - 1: Name = nnote-container Image URI = ${ERC\u0026rsquo;s URI from above}:latest Essential container = Yes Container port = 80 ECS Cluster Service\nFrom AWS Elastic Container Service console -\u0026gt; Cluster Click on our cluster nnote-us-east-1-cluster -\u0026gt; under Services tab -\u0026gt; Create Configure: Launch type = FARGATE Application type = Service Family = nnote-td Service name = nnote-service Create If everything works as intented, our task will turn 1/1 shortly\nNothing good come easily. Something wrong with my image that cause my task to exit. Time for some troubleshootings. 3b. Debug: I went back to my local computer to run docker run to see if my image is still working properly and receive this error Error \u0026quot;Access denied for user 'root'@'74.212.237.134' (using password: YES)\u0026quot; means db authentication isn\u0026rsquo;t configured properly. I quickly check __init__.py and found my error. I rebuilt the container and run it locally. The error has gone. I push it to my ECR Repo and manually redeploy the Cluster service:\nNavigate AWS Elastic Container Service console -\u0026gt; Cluster nnote-us-east-1-cluster -\u0026gt; Services -\u0026gt; nnote-service Switch to Tasks tab -\u0026gt; Under Update service drop-down arrow \u0026gt; select Force new deployment Finger cross\u0026hellip; Voila! The image seems to be interacting with the Database without any issue now. 3c. Testing: Let\u0026rsquo;s test if nnote-app, first, we need the container public IP address\nAmazon Elastic Container Service -\u0026gt; Clusters -\u0026gt; nnote-us-east-1-cluster -\u0026gt; Services -\u0026gt; nnote-service Switch to Tasks tab -\u0026gt; Select task -\u0026gt; switch to Networking Copy Public Ip address Paste on my browser and add port 5000 I can access my login screen! Let\u0026rsquo;s try sign-up\u0026hellip; Then add some note\u0026hellip; Let\u0026rsquo;s check our nnote_database.user on AWS RDS Database by navigate to MySQL Workbench \u0026gt; expand nnote_database \u0026gt; select user \u0026gt; enter this command SELECT * FROM nnote_database.user How about nnote_database.note? SELECT * FROM nnote_database.note Everything seems to be working as intented. But deploying this project involves a lot of manual work and the authentication between services are not secure. Let\u0026rsquo;s streamline the CI/CD and enhance the authentication method at Stage 4\nStage 4: Hosting on AWS using Terraform I automate Stage 3 using Terraform but I still have to build and push Docker manually. I add my own VPC in network.tf.\n# terraform/network.tf resource \u0026#34;aws_vpc\u0026#34; \u0026#34;nnote\u0026#34; { cidr_block = var.vpc_cidr enable_dns_hostnames = true tags = { Name = var.vpc_name } } resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;nnote\u0026#34; { vpc_id = aws_vpc.nnote.id tags = { Name = \u0026#34;nnote_igw\u0026#34; } } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;s1\u0026#34; { vpc_id = aws_vpc.nnote.id cidr_block = var.s1_cidr availability_zone = var.s1_az tags = { Name = \u0026#34;nnote_vpc_s1\u0026#34; } } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;s2\u0026#34; { vpc_id = aws_vpc.nnote.id cidr_block = var.s2_cidr availability_zone = var.s2_az tags = { Name = \u0026#34;nnote_vpc_s2\u0026#34; } } resource \u0026#34;aws_route_table\u0026#34; \u0026#34;nnote\u0026#34; { vpc_id = aws_vpc.nnote.id route { cidr_block = \u0026#34;0.0.0.0/0\u0026#34; gateway_id = aws_internet_gateway.nnote.id } tags = { Name = \u0026#34;nnote_rt\u0026#34; } depends_on = [aws_internet_gateway.nnote] } resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;assoc1\u0026#34; { route_table_id = aws_route_table.nnote.id subnet_id = aws_subnet.s1.id } resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;assoc2\u0026#34; { route_table_id = aws_route_table.nnote.id subnet_id = aws_subnet.s2.id } and update variables.tf\n# terraform/variables.tf # NETWORK variable \u0026#34;vpc_name\u0026#34; { default = \u0026#34;nnote_vpc\u0026#34; type = string } variable \u0026#34;vpc_cidr\u0026#34; { default = \u0026#34;10.0.0.0/16\u0026#34; type = string } variable \u0026#34;s1_cidr\u0026#34; { default = \u0026#34;10.0.1.0/24\u0026#34; type = string } variable \u0026#34;s1_az\u0026#34; { default = \u0026#34;us-east-1a\u0026#34; type = string } variable \u0026#34;s2_cidr\u0026#34; { default = \u0026#34;10.0.2.0/24\u0026#34; type = string } variable \u0026#34;s2_az\u0026#34; { default = \u0026#34;us-east-1b\u0026#34; type = string } variable \u0026#34;vpc_sg_name\u0026#34; { default = \u0026#34;nnote_vpc_sg\u0026#34; type = string } This stage went smoothly and I didn\u0026rsquo;t encounter any issue which I was happy to cut it short to the Final Stage\nStage 5: Automate deployment using Github Actions The goal of this stage is to transform my app into a ready-to-deploy application that can be use by anyone\n5a. Github Actions Workflow: Copy repo to Github Actions Runner AWS OICD to authenticate runner with AWS Create S3 bucket to store Terraform backend tfstate file Deploy Terraform Capture Terraform Output Login AWS ECR Build and Push Docker Image to AWS ECR Force ECS Cluster Service\u0026rsquo;s redeployment Roll back if fail() so that I don\u0026rsquo;t have to clean up AWS Resources if something goes wrong, this is optional and I figure it would make my life so much better for testing AWS OICD\nI read [4] [5] to setup AWS OICD role. It seems very straight forward so I won\u0026rsquo;t document these steps. You will need to configre this from your AWS IAM using your Github token.\nInjecting Secrets to Docker Image\nI configure deploy.yml to pick up DB_NAME, DB_USERNAME, and DB_PASSWORD from the Github Actions Secrets.\nTo setup GHA Secrets\nNavigate to your Github repo Select Settings \u0026gt; Secrets and variables Configure 3 variable with the exact name as below: DB_NAME= your-db-name (can be anything, this does not matter) DB_USERNAME = your-db-username DB_PASSWORD = your-db-password Injecting DB_HOST (RDS endpoints) to Docker Image\nDocker Image also needs the endpoints which is resulted from the Terraform\u0026rsquo;s deployment. I output endpoint using output.tf, capture and exported it into Github Actions Environment.\n# terraform/output.tf output \u0026#34;db_endpoint\u0026#34; { value = aws_db_instance.nnotedb.endpoint } I also output ecr_uri, cluster_arn, service_name, and region for login ECR, push Docker image and reploy cluster service later on.\n# .github/workflows/deploy.yml - name: Export Terraform output id: tf if: success() run: | echo \u0026#34;TF_EURI=$(terraform output -raw ecr_uri)\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV echo \u0026#34;TF_CARN=$(terraform output -raw cluster_arn)\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV echo \u0026#34;TF_SN=$(terraform output -raw service_name)\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV echo \u0026#34;TF_REGION=$(terraform output -raw region)\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV HOST=$(echo \u0026#34;$(terraform output -raw db_endpoint)\u0026#34; | cut -d \u0026#39;:\u0026#39; -f1) echo \u0026#34;TF_ENDPOINT=$HOST\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV working-directory: ./terraform Login AWS ECR\n- name: Log in to ECR run: aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin $TF_EURI Build and Push Docker Along with 4c and 4d, I inject these 4 variables while building the Docker image\n- name: Build Docker run: | docker build \\ --build-arg DB_USERNAME=${{ secrets.DB_USERNAME }} \\ --build-arg DB_PASSWORD=${{ secrets.DB_PASSWORD }} \\ --build-arg DB_NAME=${{ secrets.DB_NAME }} \\ --build-arg DB_HOST=${{ env.TF_ENDPOINT }} \\ -t nnote-app . - name: Tag Docker run: docker tag nnote-app:latest $TF_EURI:latest - name: Push Docker run: docker push $TF_EURI:latest Redeploy Cluster Service\n- name: Force Redeployment ECS run: aws ecs update-service --cluster $TF_CARN --service $TF_SN --force-new-deployment --region $TF_REGION Roll back if fail()\n- name: Auto clean up if Apply failed if: failure() run: | terraform destroy --auto-approve -var=\u0026#39;db_name=\u0026#34;a\u0026#34;\u0026#39; -var=\u0026#39;db_password=\u0026#34;b\u0026#34;\u0026#39; -var=\u0026#39;db_username=\u0026#34;c\u0026#34;\u0026#39; aws s3 rm s3://nnote-tfstate-031225 --recursive aws s3 rb s3://nnote-tfstate-031225 --force working-directory: ./terraform 5b. Debugging: I ran into a couple of issues\nIssue 1: RDS Endpoint from Terraform output contains :3306 Solution: cut the :3306 from the string\n# .github/workflows/deploy.yml HOST=$(echo \u0026#34;$(terraform output -raw db_endpoint)\u0026#34; | cut -d \u0026#39;:\u0026#39; -f1) echo \u0026#34;TF_ENDPOINT=$HOST\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV Issue 2: Container cannot authenticate with Database unless I use MySQL Workbench to drop the schemas and redeploy Cluster Service Solution: remove db_name = var.db_name when create aws_rds_instance\nBefore\u0026hellip;\nresource \u0026#34;aws_db_instance\u0026#34; \u0026#34;nnotedb\u0026#34; { db_name = var.db_name allocated_storage = 20 engine = \u0026#34;mysql\u0026#34; engine_version = \u0026#34;8.0\u0026#34; instance_class = var.db_instance_class username = var.db_username password = var.db_password vpc_security_group_ids = [aws_security_group.nnote_vpc_sg.id] db_subnet_group_name = aws_db_subnet_group.nnotedb.name skip_final_snapshot = true publicly_accessible = true } After\u0026hellip;\nresource \u0026#34;aws_db_instance\u0026#34; \u0026#34;nnotedb\u0026#34; { allocated_storage = 20 engine = \u0026#34;mysql\u0026#34; engine_version = \u0026#34;8.0\u0026#34; instance_class = var.db_instance_class username = var.db_username password = var.db_password vpc_security_group_ids = [aws_security_group.nnote_vpc_sg.id] db_subnet_group_name = aws_db_subnet_group.nnotedb.name skip_final_snapshot = true publicly_accessible = true } References [1] MYSQL vs SQLite https://www.greengeeks.com/blog/sqlite-vs-mysql/#:~:text=Ultimately%2C%20SQLite%20is%20a%20lightweight,go%2Dto%20for%20RDBMS%20solutions\n[2] Gunicorn https://flask.palletsprojects.com/en/stable/deploying/gunicorn/\n[3] AWS ECS https://docs.aws.amazon.com/AmazonECS/latest/developerguide/create-container-image.html\n[4] AWS OICD https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc.html\n[5] GitHub Actions and AWS OICD https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc.html\n","permalink":"http://localhost:1313/posts/flask-note-w-aws-terraform/","summary":"Deploy infrastructure using Terraform ¹, AWS Lambda ², and GitHub Actions ³. During a conversation with a close friend in Australia, I was inspired by the idea of using a lightweight framework like Flask to build a cloud application. Thanks @terryduong for the inspiration and support for this project.\nCheck out my project on GitHub\nStage 0: Game plan I split this project into five stages.\nStage 1: Creating the app Stage 2: Host locally Stage 3: Hosting on AWS using Console Stage 4: Hosting on AWS using Terraform Stage 5: Automate the deployment using Terraform and Github Actions A Reference List at the end of this page includes all the documents I consulted throughout this project.","title":"Hosting a Python Flask Note Using AWS Services, Terraform, Github Actions"},{"content":" Intro: Recently, I explored a simple yet powerful way to achieve image processing using AWS Lambda and AWS Rekognition. With Machine Learning on the rise, I think it would be a cool project for me to get my hands on this topic. In this post, I’ll walk through a scalable and cost-effective approach: using AWS Lambda to process images stored in an S3 bucket, analyze their content with Rekognition to identify objects, faces, or text, and then store the extracted metadata in another S3 bucket for further use.\nTerraform: 1. S3 I created two S3 buckets: one for input images and one for storing Rekognition metadata and resized images. While it is possible to use a single bucket, I would not recommend it, as putting a new image into the same bucket that triggers the Lambda function could cause the application to enter an infinite loop.\n# S3 resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;input_bucket\u0026#34; { bucket = \u0026#34;serverless-image-input-bucket\u0026#34; } resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;output_bucket\u0026#34; { bucket = \u0026#34;serverless-image-out-bucket\u0026#34; } 2. Lambda I created a Lambda function using Python 3.9 and used a customized layer from the user \u0026ldquo;Klayers\u0026rdquo; to import the PIL library.\n# Lambda Function resource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;image_processor\u0026#34; { filename = \u0026#34;lambda_function.zip\u0026#34; function_name = \u0026#34;image-processing-lambda\u0026#34; role = aws_iam_role.lambda_role.arn handler = \u0026#34;lambda_function.lambda_handler\u0026#34; runtime = \u0026#34;python3.9\u0026#34; timeout = 10 source_code_hash = filebase64sha256(\u0026#34;lambda_function.zip\u0026#34;) layers = [\u0026#34;arn:aws:lambda:us-east-1:770693421928:layer:Klayers-p39-pillow:1\u0026#34;] environment { variables = { OUTPUT_BUCKET = aws_s3_bucket.output_bucket.id } } } resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;allow_s3\u0026#34; { statement_id = \u0026#34;AllowS3Invoke\u0026#34; action = \u0026#34;lambda:InvokeFunction\u0026#34; principal = \u0026#34;s3.amazonaws.com\u0026#34; function_name = aws_lambda_function.image_processor.function_name source_arn = aws_s3_bucket.input_bucket.arn } 3. Permissions and Roles I created a custom inline IAM role for the Lambda function and granted it the necessary permissions to execute tasks involving both S3 buckets and the CloudWatch LogGroup.\nresource \u0026#34;aws_iam_role\u0026#34; \u0026#34;lambda_role\u0026#34; { name = \u0026#34;lambda-image-processing-role\u0026#34; assume_role_policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [{ Action = \u0026#34;sts:AssumeRole\u0026#34; Effect = \u0026#34;Allow\u0026#34; Principal = { Service = \u0026#34;lambda.amazonaws.com\u0026#34; } }] }) } resource \u0026#34;aws_iam_policy_attachment\u0026#34; \u0026#34;lambda_policy\u0026#34; { name = \u0026#34;lambda-s3-rekognition-policy\u0026#34; roles = [aws_iam_role.lambda_role.name] policy_arn = \u0026#34;arn:aws:iam::aws:policy/AmazonS3FullAccess\u0026#34; } resource \u0026#34;aws_iam_policy_attachment\u0026#34; \u0026#34;lambda_rekognition_policy\u0026#34; { name = \u0026#34;lambda-rekognition-policy\u0026#34; roles = [aws_iam_role.lambda_role.name] policy_arn = \u0026#34;arn:aws:iam::aws:policy/AmazonRekognitionFullAccess\u0026#34; } resource \u0026#34;aws_iam_role_policy\u0026#34; \u0026#34;logging\u0026#34; { name = \u0026#34;lambda-image-processing-logging-policy\u0026#34; role = aws_iam_role.lambda_role.name policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34;, Statement = [{ Effect = \u0026#34;Allow\u0026#34;, Action = [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], Resource = [\u0026#34;arn:aws:logs:*:566027688242:log-group:*:log-stream:*\u0026#34;] }] }) } 4. S3 Triggers Finally, I created a trigger event so that every time a new object is added to the input S3 bucket, it will invoke the Lambda function to process the image. The depends_on attribute ensures that the Lambda function is created before the trigger is assigned.\nresource \u0026#34;aws_s3_bucket_notification\u0026#34; \u0026#34;s3_trigger\u0026#34; { bucket = aws_s3_bucket.input_bucket.id lambda_function { lambda_function_arn = aws_lambda_function.image_processor.arn events = [\u0026#34;s3:ObjectCreated:*\u0026#34;] } depends_on = [aws_lambda_function.image_processor] # Ensure the lambda function is created first } Deploy: 1. AWS CLI I created an AWS CLI account with full permissions since I will deploy the Terraform files remotely from my local computer.\n2. Terraform CLI: terraform fmt # Format main.tf to ensure consistency in syntax terraform validate # Validate the configuration terraform plan # Review what I\u0026#39;m about to deploy terraform apply --auto-approve # Deploy main.tf Testing: The test is straightforward. I upload a picture of a random animal into the input S3 bucket. The presence of a new object in this bucket triggers the Lambda function, which also logs an entry in the CloudWatch LogGroup. The Lambda function performs two tasks:\nMakes a copy of the original picture, resizes it, and stores it in the output bucket. Uses AWS Rekognition to analyze the image content and outputs the metadata as a JSON file into the output bucket. But first thing first, I will check if my terraform file has been deployed correctly by running this command:\nterraform state list It works perfectly, and here is the result. Refinement: I have only scratched the surface of AWS Rekognition\u0026rsquo;s use cases. This small application can be scaled up to handle a large volume of images in bulk where Rekognition has a lot more to offer.\nResources and Credits Klayer https://github.com/keithrozario/Klayers Rekognition Python https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/ex-lambda.html\nCheck out this project on my GitHub.\n","permalink":"http://localhost:1313/posts/rekcognition/","summary":"Intro: Recently, I explored a simple yet powerful way to achieve image processing using AWS Lambda and AWS Rekognition. With Machine Learning on the rise, I think it would be a cool project for me to get my hands on this topic. In this post, I’ll walk through a scalable and cost-effective approach: using AWS Lambda to process images stored in an S3 bucket, analyze their content with Rekognition to identify objects, faces, or text, and then store the extracted metadata in another S3 bucket for further use.","title":"Rekognition"},{"content":" Check out my project on GitHub\nThe Goal: The idea is to use Infrastructure as Code (IaC) to quickly spin up a simple yet secure virtual network on AWS. Terraform CLI is my tool of choice for managing it all, so I can automate the setup and ensure everything stays organized and reproducible.\nWhat’s Inside the Infrastructure: A VPC in the us-west-1 region with a CIDR block of 10.0.0.0/16—this is the foundation of the network. A Public Subnet (10.0.1.0/24) connected to the internet via an Internet Gateway. The cool thing is that all the traffic from this subnet can flow freely to the internet, thanks to a simple Public Route Table. A Private Subnet (10.0.2.0/24) that’s a little more closed off—this one doesn’t have direct internet access. A NAT Gateway placed in the public subnet, with an Elastic IP. This allows instances in the private subnet to reach the internet for things like updates, but without exposing them to the outside world. We handle all this securely with a Private Route Table. Testing: I launched a t3.micro Linux instance in both the Public Subnet and Private Subnet. First, I SSH into the Public Instance using its public IP address. Then, I recreated my private key and stored it on the Public Instance, before SSH into the Private Instance using its private IP address. From Private Instance, I couldn\u0026rsquo;t ping google.com nor able to yum update it. The first thing I check was my Route Tables and voila, looks like I forgot to associate my Private RT with Private subnet, therefore AWS has to created a default RT and associated it with the Private Subnet which is not what we want because our instance cannot reach NAT Gateway therefore no connection. I went back to my main.tf and added this block\u0026hellip; # Associate Private Route Table with Private Subnet\rresource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;private\u0026#34; {\rsubnet_id = aws_subnet.terraform_private_subnet_1.id\rroute_table_id = aws_route_table.terraform_rt_private_1.id\r} \u0026hellip;rerun terraform\u0026hellip;\nterraform plan\rterraform apply --auto-approve \u0026hellip;make sure Private Route Table is in placed, as you can see, the Private Subnet is now properly\u0026hellip; \u0026hellip; I can ping and update Private Instance as the traffic has succesfully flows through the Private Route Table and reach NAT Gateway! Key Pairs and Security Notes: To keep things simple, I generated a public/private key pair on my local computer using PowerShell: ssh-keygen -t rsa -b 2048 -f C:\\path\\to\\your\\keyfile I used this key pair for both the Public and Private Instances. To SSH into the private instance, I created a new private key on the Public Instance, pasted it in, and used it for access. I know this isn’t the most secure way to handle things, but it helped me keep it simple for this demo. Room for Improvement: Going forward, it would be much better to use AWS Secrets Manager to securely store and manage secrets. This would help scale things more securely and take the guesswork out of key management. ","permalink":"http://localhost:1313/posts/my-first-terraform/","summary":"Check out my project on GitHub\nThe Goal: The idea is to use Infrastructure as Code (IaC) to quickly spin up a simple yet secure virtual network on AWS. Terraform CLI is my tool of choice for managing it all, so I can automate the setup and ensure everything stays organized and reproducible.\nWhat’s Inside the Infrastructure: A VPC in the us-west-1 region with a CIDR block of 10.0.0.0/16—this is the foundation of the network.","title":"My First Terraform"},{"content":" Hey there! Welcome to my tech blog. I’m excited to share a little project: automating Shopify orders data into a database warehouse and setting up the connection for analysis. I wanted a simple way to fetch data and get real-time insights without all the manual effort. So, I set up AWS Event Scheduler to trigger data retrieval, used AWS Lambda for serverless processing, and chose DynamoDB for storage. To connect DynamoDB to Microsoft Power BI, I used CDATA Universal Software to create an ODBC driver.\nI\u0026rsquo;m sure there are more sophisticated solutions out there and this is my humble approach, but I hope you find this useful! Let’s turn Shopify orders into awesome business insights together! Setup AWS Lambda: I created a Lambda function to fetch data from Shopify and store it in DynamoDB. AWS Event Scheduler: I use Event Scheduler to trigger the Lambda function daily. AWS DynamoDB: While a relational database might be better for a data warehouse, I opted for DynamoDB for its simplicity and cost-effectiveness. CDATA Amazon DynamoDB ODBC Driver: I configured my DynamoDB ODBC driver with CDATA since they offer free trials. This driver is essential for pulling data into Power BI since it doesn’t support DynamoDB out-of-the-box. Power BI: Finally, I set up the ODBC driver as a data source in Power BI to pull in my data and start visualizing! CloudWatch: I also set up CloudWatch for logging to keep track of everything. Improvements Explore a Relational Database: Consider using Amazon RDS instead of DynamoDB for enhanced data management and querying capabilities, leading to more efficient analysis. Improve Error Handling: Implement more robust error handling in the AWS Lambda function to ensure better resilience during data retrieval. Flexible Scheduling: Adjust the data retrieval schedule based on business needs, rather than sticking to a fixed daily trigger, to optimize resource usage. Data Validation and Transformation: Incorporate processes for data validation and transformation before loading data into DynamoDB to improve overall data quality. Enhance Power BI Dashboards: Add more interactive elements and deeper analytics in Power BI to provide valuable insights for decision-making. Check out this project on my GitHub.\nThank you for visiting!\n","permalink":"http://localhost:1313/posts/shopify-daily-report/","summary":"Hey there! Welcome to my tech blog. I’m excited to share a little project: automating Shopify orders data into a database warehouse and setting up the connection for analysis. I wanted a simple way to fetch data and get real-time insights without all the manual effort. So, I set up AWS Event Scheduler to trigger data retrieval, used AWS Lambda for serverless processing, and chose DynamoDB for storage. To connect DynamoDB to Microsoft Power BI, I used CDATA Universal Software to create an ODBC driver.","title":"Fetching Shopify API using AWS Lambda and DynamoDB"},{"content":" Welcome to my tech blog! Today, I’m excited to take you on my cloud journey with a project which I was inspired from real-world challenge: integrating SMS notifications into a software that had numerous restrictions. I needed a solution to notify users whenever the database was updated. Instead of wrestling with the software’s limitations, I decided to create a microservice that could handle SMS notifications independently. This way, the software team can invoke an API to send SMS messages without worrying about the underlying code and integration complexities.\nSetup In this project, I use Twilio for sending text messages, combined with AWS Lambda, AWS API Gateway, and AWS Route53 for domain and API mapping. Here’s how I set it up:\nTwilio Setup: I bought a number from Twilio, loaded a few bucks into the account, and created a Lambda function to send SMS whenever the API is invoked using the Twilio phone number. API Gateway Configuration: I created a POST method API with a custom domain name and mapped it to my Lambda function. Route53 Configuration: I updated an A record on Route53 with my API Gateway\u0026rsquo;s ARN to handle the domain and API mapping seamlessly. Check out my project on GitHub\nImprovement There are few improvements that I would like to integrate in the future are:\nAuthentication: I could add API key authentication through custom headers or integrate Okta for more advanced identiy management. This will ensure authorized users can trigger the SMS functionality. Loggin and Monitoring: Using AWS CloudWatch would help me keep track of performance and quickly resolve any issues. Reliability: Implementing retru mechanisms and error handling strategies into my lambda function would make the service more reliable, ensuring messages are sent even during temporary failures. By the end of this project, I happy with this robust and scalable SMS notification service that can be easily integrated into any application. Let’s get started!\n","permalink":"http://localhost:1313/posts/twilio-microservice/","summary":"Welcome to my tech blog! Today, I’m excited to take you on my cloud journey with a project which I was inspired from real-world challenge: integrating SMS notifications into a software that had numerous restrictions. I needed a solution to notify users whenever the database was updated. Instead of wrestling with the software’s limitations, I decided to create a microservice that could handle SMS notifications independently. This way, the software team can invoke an API to send SMS messages without worrying about the underlying code and integration complexities.","title":"Send SMS with Twilio using AWS Lambda and API Gateway"},{"content":"Intro Hey there! When I decided to launch my website, I wanted a reliable and scalable solution. After buying my domain from GoDaddy, I turned to Amazon Web Services (AWS) for hosting. It’s been an exciting journey figuring out the best setup.\nI use Amazon S3 to store my website files, Route 53 for domain name system (DNS) management, and CloudFront to deliver content quickly and securely. It wasn’t all smooth sailing, but the flexibility and power of AWS made it worth it. Join me as I share the highs and lows of getting my site online using GoDaddy and AWS services!\nStep 1: Purchase my first domain Initially, I planned to buy my domain directly from AWS, but Route 53 doesn’t support the .one top-level domain. So, I ended up purchasing it from GoDaddy instead. This choice means I need to update the Nameserver (NS) records in the GoDaddy console—an extra step compared to buying directly from AWS, where this would be unnecessary.\nIf you\u0026rsquo;re curious about which top-level domains AWS supports, you can check out the full list here[https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/registrar-tld-list.html].\nHaving a domain on GoDaddy and hosting on AWS means I have to manage the Nameservers accordingly.\nStep 2: Route53 NameServers First, I need to create a Route 53 Hosted Zone. AWS will provide four NameServers (NS) by default. I’ll use these to update the NS records on my GoDaddy domain.\nCreating a New Hosted Zone in Route 53 When you buy a domain from GoDaddy, it comes with two default NS records pointing to GoDaddy servers. Since I will need to configure CloudFront in the end, I will have Godaddy gives up DNS administration to AWS Route 53.\nSteps to Generate New AWS NameServers from Route 53: Sign in to AWS Management Console and navigate to Route53. On the left panel, select Hosted zones. Choose Create hosted zone. Under Domain name, I entered nvo.one Under Type, I left it unchanged as Public hosted zone. Under Tags, choose Add tag, with Key is \u0026ldquo;Website\u0026rdquo; and Value is \u0026ldquo;www.nvo.one\u0026rdquo;* to keep track of my resources later on. Choose Create Hosted Zone Route 53 will automatically generate a new set of NS records (four in total). Keep this tab open, as you\u0026rsquo;ll need these values for the next step. Update GoDaddy Domain\u0026rsquo;s NS Next, I need to update my GoDaddy domain’s NS records to use the AWS NameServers.\nSteps to Update GoDaddy Domain\u0026rsquo;s NS Records: Sign in to Godaddy.com console Choose the \u0026ldquo;Nine dots\u0026rdquo; next to my Profile to expand the nav menu. Choose Domains Highlight my domain name, choose the \u0026ldquo;3 dots\u0026rdquo; to expand the settings bar and choose Edit DNS Under my domain Domain Settings menu bar, choose Nameservers Choose Change Nameservers Confirm the change by choosing Yes to replace the GoDaddy NS records. Go back to your AWS Route 53 tab to copy each NS record and paste them into the GoDaddy console. By default, GoDaddy provides only two NS records, so you’ll need to add two more to match the four provided by AWS Route 53. Step 3: Request a SSL certificate My website requires a public certificate so Amazon CloudFront can enforce HTTPS, ensuring all connections between CloudFront and viewers are encrypted.\nHowever, CloudFront can only use certificates created in the US East (N. Virginia) region. If an SSL certificate is created in any other region, CloudFront won\u0026rsquo;t be able to use it. Here’s how I create my free AWS-provided SSL certificate:\nSign in to AWS Management Console and navigate to ACM On the left panel, select Request certificate Keep the default selection *Request a public certificate, then choose Next In the Domain names section, I entered my domain name nvo.one In the Validation method, I choose DNS validation In the Key algorithm section, I choose RSA 2048 In the Add tags sections, I create a tag name \u0026ldquo;Website\u0026rdquo; with value is my website \u0026ldquo;www.nvo.one\u0026rdquo; to keep track of my resource in AWS Choose Request to be taken to the Certificates page. Once the new certificate appears in Pending status, choose the certificate ID, and on the certificate details page, choose Create record in Route53 to add the CNAME records to my domains, I used the same Route53 Hosted Zone above to create records in. It took less than 30\u0026rsquo; to verify my domain, and the status will change to Issued when it\u0026rsquo;s ready to be used. Step 4: Create and Set Up S3 bucket I will need 2 buckets: one is for my subdomain www.nvo.one which will contains my static webside, and another one for my root nvo.one which will point to my subdomain bucket.\nCreate and Set Up my sub-domain bucket I need to create a bucket to store my static website content\nSign in to AWS Management Console and navigate to S3 On the left panel, choose Buckets Choose Create bucket I name it www.nvo.one Under Block public access (bucket settings), untick Block all public access, choose confirm so that my bucket is accessible by public Under Bucket policy, choose Edit, I copy the policy from AWS, modify my bucket name and save it { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::www.nvo.one/*\u0026#34; } ] } Under Static website hosting, I choose Enable For Hosting type, I choose Host a static website For Index document, I entered \u0026ldquo;index.html\u0026rdquo; For Error document, I left it blank for now Now the bucket is ready to use, I copy my website in To test it, choose the bucket, on the nav menu, choose Properties, under Static website hosting, copy the bucket endpoints and paste it on the browser. Create and Set Up your root domain Sure, here’s a rephrased version of that section:\nTo enable users to access my website without typing www., I need to create an additional S3 bucket. This bucket will handle requests and redirect users to my main S3 bucket where the website content is hosted.\nCreate bucket Name it nvo.one (without www.) Choose Properties Under Static website hosting, choose Edit. Choose Redirect requests fr an object. In the Host name box, I entered my subdomain nvo.one. For Protocol, choose HTTPS. Choose Save changes Step 5: Create an Amazon CloudFront distribution for my subdomain www.nvo.one I will need to also 2 CloudFront distribution: one for subdomain and one for root where they both using my SSL Certification.\nSubdomain distribution Sign in to AWS Management Console and navigate to CloudFront. Choose Create Distribution. Under Origin, for Origin domain, I choose my Amazon S3 bucket that I created for my subdomain (ww.nvo.one).\\ For the settings under Default Cache Behavior Settings, under Viewer, set Viewer protocol policy to Redirect HTTP to HTTPS and accept the default values for the rest. For the fields under Settings, do the following:\\ Choose Add item for Alternate domain (CNAME) - optional and enter my subdomain name For Custom SSL Certificate, choose the certificate created above For Default root object text box, type in index.html I leave everything else default and choose Create distribution Once the distribution created, I monitor the Status column, it will takes sometimes to change from In progress to Deployed where CloudFront will distribute my website to the world. How excited! Root domain distribution I also want my root domain is distributed CDN to the world to server all the request to my root and of course the response will be redirected from my root bucket.\nSign in to AWS Management Console and navigate to CloudFront Choose Create Distribution. Under Origin, for Origin Domain Name, I enter my root S3 bucket endpoints. For the settings under Default Cache Behavior Settings\\ Under Viewer, set Viewer protocol policy to Redirect HTTP to HTTPS Set Cache settings to CachingDisabled. For the fields under Settings, do the following:\\ Choose Add item for Alternate domain (CNAME) - optional and enter my subdomain name For Custom SSL Certificate, choose the certificate created above Choose Create Distribution Give it sometimes, the Status will change from In Progress to Deployed that is when the distribution is ready to be used. Step 6: Route DNS traffic from my Domain name to my CloudFront distributions As of now, I should have a working domain name that is ready to be accessed by the world and 2 CloudFront distributions which are ready to distribute my content (nvo.one and www.nvo.one). Next, I need to create a record for each of the distribution group to Route53 Hosted Zone where my domain name is using the same NS records.\nFor subdomain Sign in to AWS Management Console and navigate to Route53 On the left panel, choose Hosted zones Under Hosted zones, choose my hosted zone nvo.one Choose Create record. Choose Switch to wizard. Choose Simple routing and choose Next. In Record name, I typed www in front of the default value In Record type, choose A - Routes traffic to an IPv4 address and some AWS resources in Value/Route traffic to, choose Alias to CloudFront distribution. Choose my nvo.one CF distribution. For Evaluate target health, I choose No. Choose Define simple record. For root Sign in to AWS Management Console and navigate to Route53 On the left panel, choose Hosted zones Under Hosted zones, choose my hosted zone www.nvo.one Choose Create record. Choose Switch to wizard. Choose Simple routing and choose Next. In Record name, I typed www in front of the default value In Record type, choose A - Routes traffic to an IPv4 address and some AWS resources in Value/Route traffic to, choose Alias to CloudFront distribution. Choose my nvo.one CF distribution. For Evaluate target health, I choose No. Choose Define simple record. Step 7: FAQ Invalidation There are often a few times after updating my subdomain bucket, my website doesn\u0026rsquo;t really update. Invalidation would help clear out the cache at AWS\u0026rsquo;s edge location and force the edge location to pull from our S3 bucket again. This will guarantee user will see the updates immediately. But it will result a small charge which can accumulate over the time.\nWhich will leads to another question that how can I know that my bucket already receive the update and will display it correctly before the edge location TTL runs out and sync up. By using S3 Static Web Hosting endpoint.\nS3 Static Web Hosting endpoint Once the new update is copied to the S3 bucket, one way that we can see how to website looks like even before it will be pull to the CloudFront Edge\u0026rsquo;s location is to use Statuc Web Hosting endpoint.\nSign to AWS Management Console, navigate to S3 Choose to subdomain buckets (www.nvo.one) Choose Properties, scroll to the end Copy the Static Web Hosting endpoints and paste it into any browser ","permalink":"http://localhost:1313/posts/007-website-hosting/","summary":"Intro Hey there! When I decided to launch my website, I wanted a reliable and scalable solution. After buying my domain from GoDaddy, I turned to Amazon Web Services (AWS) for hosting. It’s been an exciting journey figuring out the best setup.\nI use Amazon S3 to store my website files, Route 53 for domain name system (DNS) management, and CloudFront to deliver content quickly and securely. It wasn’t all smooth sailing, but the flexibility and power of AWS made it worth it.","title":"How I host my website on AWS using Domain from Godaddy.com"},{"content":"Project name: Croddy\nNest Camera Guide: https://developers.google.com/nest/device-access/get-started Register Device Access Accept $5 one time fee https://console.nest.google.com/device-access/ Activate Supported Device Activate and setup device with non-commercial google account Setup Google Cloud Platform Use the link on top of the article, scroll down to Setup Google Cloud Platform and click on Enable the API and Get an OAuth 2.0 Client ID Select Webserver \u0026gt; Enter https://www.google.com \u0026gt; copy Client ID and download the file Create a Device Access project Go to Device Access \u0026gt; Create new project \u0026gt; paste Client ID \u0026gt; disable event Once done, copy Project ID down Now you have to link your developer account with user account\nLink your account PCM - Get Authorization Code https://nestservices.google.com/partnerconnections/project-id/auth?redirect_uri=https://www.google.com\u0026amp;access_type=offline\u0026amp;prompt=consent\u0026amp;client_id=oauth2-client-id\u0026amp;response_type=code\u0026amp;scope=https://www.googleapis.com/auth/sdm.service Go to Google Cloud Platform \u0026gt; OAuth consent screen \u0026gt; Under Test User \u0026gt; add yourself in replace project-id (from Device Access Project ID) replace oath2-client-id with OAuthClient ID from Google Cloud Crendetial Copy and paste to google Allow and follow next Once done, it will go to www.google.com, copy the URL The Authorization code is in the URL: https://www.google.com/?code=4/0AfJohXlvngYWOp06a-e68hfU7jEOyrMeUyPlA2AVfs-og4i172VeEBzLLgP7UsiWxGkXrA\u0026amp;scope=https://www.googleapis.com/auth/sdm.service the code start after code= and end \u0026amp;scope Get Access Token Open terminal, replace oauth2-client-id with the OAuth2 Client ID from your Google Cloud Credentials oauth2-client-secret with Client Secret from your Google Cloud Credentials authorization-code with the code you received in the previous stepvariable and run curl -L -X POST \u0026#39;https://www.googleapis.com/oauth2/v4/token?client_id=oauth2-client-id\u0026amp;client_secret=oauth2-client-secret\u0026amp;code=authorization-code\u0026amp;grant_type=authorization_code\u0026amp;redirect_uri=https://www.google.com\u0026#39; if it give you error, run Remove-item alias:curl if it gives you Error 411 (Length Required), add this to the end of your curl -H \u0026#39;Content-Length: 0\u0026#39; it should return 2 tokens: access token and refresh token { \u0026#34;access_token\u0026#34;: \u0026#34;access-token\u0026#34;, \u0026#34;expires_in\u0026#34;: 3599, \u0026#34;refresh_token\u0026#34;: \u0026#34;refresh-token\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;https://www.googleapis.com/auth/sdm.service\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;Bearer\u0026#34; } Make a device list call IMPORTANT: This call is required to complete the authorization, with out this call, the authorization project will not appear in PCM and events will not be published curl -X GET \u0026#39;https://smartdevicemanagement.googleapis.com/v1/enterprises/project-id/devices\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -H \u0026#39;Authorization: Bearer access-token\u0026#39; Replace project-id with Project ID find in Deviec Access console Replace access-token with Access Token from step 6 remove space and \\ to make the curl into one line copy and paste into terminal result { \u0026#34;devices\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;enterprises/project-id/devices/device-id\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;sdm.devices.types.device-type\u0026#34;, \u0026#34;traits\u0026#34;: { ... }, \u0026#34;parentRelations\u0026#34;: [ { \u0026#34;parent\u0026#34;: \u0026#34;enterprises/project-id/structures/structure-id/rooms/room-id\u0026#34;, \u0026#34;displayName\u0026#34;: \u0026#34;device-room-name\u0026#34; } ] } ] } ","permalink":"http://localhost:1313/posts/005-google-sdm/","summary":"Project name: Croddy\nNest Camera Guide: https://developers.google.com/nest/device-access/get-started Register Device Access Accept $5 one time fee https://console.nest.google.com/device-access/ Activate Supported Device Activate and setup device with non-commercial google account Setup Google Cloud Platform Use the link on top of the article, scroll down to Setup Google Cloud Platform and click on Enable the API and Get an OAuth 2.0 Client ID Select Webserver \u0026gt; Enter https://www.google.com \u0026gt; copy Client ID and download the file Create a Device Access project Go to Device Access \u0026gt; Create new project \u0026gt; paste Client ID \u0026gt; disable event Once done, copy Project ID down Now you have to link your developer account with user account","title":"Using Google SDM API to control Google Nest Camera"},{"content":"1. Source Code https://github.com/nhatvo1502/Automate-IaC/tree/main 2. What the code does 2.1. create-resource.yml configure aws credential verify aws credential verify aws s3 permission create s3 bucket for terraform backend copy file from GH to runner verify terraform file terraform init -\u0026gt; apply 2.2 destroy-resource.yml configure aws credential copy file from GH to runner verify terraform file terraform init -\u0026gt; destroy empty s3 backend bucket delete-bucket 2.3 main.tf configure s3 as backend create an s3 bucket create a keypair using a public key from GitHub Secret AMI lookup create VPC create a subnet create a network interface create EC2 instance 3. Setup 3.1 GH clone and repo set You will need to clone my code\ngh repo clone https://github.com/nhatvo1502/Automate-IaC/tree/main When you clone a reposityory, it added as a remote of yours, under the name origin. What you need to do now (as you\u0026rsquo;re not using the my source anymore) is to rename origin to upstream and add your own origin URL:\ngit remote rename origin upstream git remote add origin http://github.com/YOU/YOUR_REPO Whenever you want to changes from my branch (which is your upstream)\ngit fetch upstream Whenever you want to commit and push to your repository\ngit add *.* git commit -m \u0026#39;YOUR_MESSAGE\u0026#39; git push 3.2 Github Secrets Create AWS CLI user with Admin Permission https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html Create GitHub Secrets https://docs.github.com/en/codespaces/managing-codespaces-for-your-organization/managing-development-environment-secrets-for-your-repository-or-organization Login to your Github Account \u0026gt; Go to your project Repository \u0026gt; Settings \u0026gt; Under Secrets and variables \u0026gt; Select Actions \u0026gt; Create an environment \u0026ldquo;Dev\u0026rdquo; \u0026gt; Create AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY 4. Github Actions 5. GitHub Action runner results: 5.1 Terraform apply Run terraform apply -auto-approve data.aws_ami.ubuntu: Reading... data.aws_ami.ubuntu: Read complete after 0s [id=ami-027a754129abb5386] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aws_instance.new_instance will be created + resource \u0026#34;aws_instance\u0026#34; \u0026#34;new_instance\u0026#34; { + ami = \u0026#34;ami-027a754129abb5386\u0026#34; + arn = (known after apply) + associate_public_ip_address = (known after apply) + availability_zone = (known after apply) + cpu_core_count = (known after apply) + cpu_threads_per_core = (known after apply) + disable_api_stop = (known after apply) + disable_api_termination = (known after apply) + ebs_optimized = (known after apply) + get_password_data = false + host_id = (known after apply) + host_resource_group_arn = (known after apply) + iam_instance_profile = (known after apply) + id = (known after apply) + instance_initiated_shutdown_behavior = (known after apply) + instance_lifecycle = (known after apply) + instance_state = (known after apply) + instance_type = \u0026#34;t3.micro\u0026#34; + ipv6_address_count = (known after apply) + ipv6_addresses = (known after apply) + key_name = \u0026#34;automate-iac-kp\u0026#34; + monitoring = (known after apply) + outpost_arn = (known after apply) + password_data = (known after apply) + placement_group = (known after apply) + placement_partition_number = (known after apply) + primary_network_interface_id = (known after apply) + private_dns = (known after apply) + private_ip = (known after apply) + public_dns = (known after apply) + public_ip = (known after apply) + secondary_private_ips = (known after apply) + security_groups = (known after apply) + spot_instance_request_id = (known after apply) + subnet_id = (known after apply) + tags = { + \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } + tags_all = { + \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } + tenancy = (known after apply) + user_data = (known after apply) + user_data_base64 = (known after apply) + user_data_replace_on_change = false + vpc_security_group_ids = (known after apply) + network_interface { + delete_on_termination = false + device_index = 0 + network_card_index = 0 + network_interface_id = (known after apply) } } # aws_key_pair.new_keypair will be created + resource \u0026#34;aws_key_pair\u0026#34; \u0026#34;new_keypair\u0026#34; { + arn = (known after apply) + fingerprint = (known after apply) + id = (known after apply) + key_name = \u0026#34;automate-iac-kp\u0026#34; + key_name_prefix = (known after apply) + key_pair_id = (known after apply) + key_type = (known after apply) + public_key = \u0026#34;MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQCJgKe/Ko/3pXldymY2/bwP70xPJzP6NpKS9iQP450sWFtc5VuG3fqaOvGMzhrS3VUseALddYALiWmgf+cgjdvASeW3eIl54y2253QTme4eb1WOZ2/MvcANBuiJFqItL7SR4xlaAuI48yS8lh7L2OIIpzy8PUV+EcPD9KGKsAicFwIDAQAB\u0026#34; + tags = { + \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } + tags_all = { + \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } } # aws_network_interface.automate-iac-aws_network_interface will be created + resource \u0026#34;aws_network_interface\u0026#34; \u0026#34;automate-iac-aws_network_interface\u0026#34; { + arn = (known after apply) + id = (known after apply) + interface_type = (known after apply) + ipv4_prefix_count = (known after apply) + ipv4_prefixes = (known after apply) + ipv6_address_count = (known after apply) + ipv6_address_list = (known after apply) + ipv6_address_list_enabled = false + ipv6_addresses = (known after apply) + ipv6_prefix_count = (known after apply) + ipv6_prefixes = (known after apply) + mac_address = (known after apply) + outpost_arn = (known after apply) + owner_id = (known after apply) + private_dns_name = (known after apply) + private_ip = (known after apply) + private_ip_list = (known after apply) + private_ip_list_enabled = false + private_ips = (known after apply) + private_ips_count = (known after apply) + security_groups = (known after apply) + source_dest_check = true + subnet_id = (known after apply) + tags = { + \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } + tags_all = { + \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } } # aws_s3_bucket.new_bucket will be created + resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;new_bucket\u0026#34; { + acceleration_status = (known after apply) + acl = (known after apply) + arn = (known after apply) + bucket = \u0026#34;automate-iac-bucket-123123123\u0026#34; + bucket_domain_name = (known after apply) + bucket_prefix = (known after apply) + bucket_regional_domain_name = (known after apply) + force_destroy = false + hosted_zone_id = (known after apply) + id = (known after apply) + object_lock_enabled = (known after apply) + policy = (known after apply) + region = (known after apply) + request_payer = (known after apply) + tags_all = (known after apply) + website_domain = (known after apply) + website_endpoint = (known after apply) } # aws_subnet.automate-iac-subnet will be created + resource \u0026#34;aws_subnet\u0026#34; \u0026#34;automate-iac-subnet\u0026#34; { + arn = (known after apply) + assign_ipv6_address_on_creation = false + availability_zone = \u0026#34;us-east-1a\u0026#34; + availability_zone_id = (known after apply) + cidr_block = \u0026#34;172.16.10.0/24\u0026#34; + enable_dns64 = false + enable_resource_name_dns_a_record_on_launch = false + enable_resource_name_dns_aaaa_record_on_launch = false + id = (known after apply) + ipv6_cidr_block_association_id = (known after apply) + ipv6_native = false + map_public_ip_on_launch = false + owner_id = (known after apply) + private_dns_hostname_type_on_launch = (known after apply) + tags = { + \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } + tags_all = { + \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } + vpc_id = (known after apply) } # aws_vpc.automate-iac-vpc will be created + resource \u0026#34;aws_vpc\u0026#34; \u0026#34;automate-iac-vpc\u0026#34; { + arn = (known after apply) + cidr_block = \u0026#34;172.16.0.0/16\u0026#34; + default_network_acl_id = (known after apply) + default_route_table_id = (known after apply) + default_security_group_id = (known after apply) + dhcp_options_id = (known after apply) + enable_dns_hostnames = (known after apply) + enable_dns_support = true + enable_network_address_usage_metrics = (known after apply) + id = (known after apply) + instance_tenancy = \u0026#34;default\u0026#34; + ipv6_association_id = (known after apply) + ipv6_cidr_block = (known after apply) + ipv6_cidr_block_network_border_group = (known after apply) + main_route_table_id = (known after apply) + owner_id = (known after apply) + tags = { + \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } + tags_all = { + \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } } Plan: 6 to add, 0 to change, 0 to destroy. aws_key_pair.new_keypair: Creating... aws_vpc.automate-iac-vpc: Creating... aws_s3_bucket.new_bucket: Creating... aws_key_pair.new_keypair: Creation complete after 0s [id=automate-iac-kp] aws_vpc.automate-iac-vpc: Creation complete after 1s [id=vpc-02606838ac1e89cea] aws_subnet.automate-iac-subnet: Creating... aws_subnet.automate-iac-subnet: Creation complete after 1s [id=subnet-0dbfc00f69e201fa7] aws_network_interface.automate-iac-aws_network_interface: Creating... aws_network_interface.automate-iac-aws_network_interface: Creation complete after 0s [id=eni-0673ed65d965c194e] aws_instance.new_instance: Creating... aws_s3_bucket.new_bucket: Still creating... [10s elapsed] aws_instance.new_instance: Still creating... [10s elapsed] aws_instance.new_instance: Creation complete after 13s [id=i-08a28371557209b0b] aws_s3_bucket.new_bucket: Creation complete after 16s [id=automate-iac-bucket-123123123] Apply complete! Resources: 6 added, 0 changed, 0 destroyed. 5.2 Terraform destroy Run terraform destroy -auto-approve data.aws_ami.ubuntu: Reading... aws_vpc.automate-iac-vpc: Refreshing state... [id=vpc-02606838ac1e89cea] aws_key_pair.new_keypair: Refreshing state... [id=automate-iac-kp] aws_s3_bucket.new_bucket: Refreshing state... [id=automate-iac-bucket-123123123] data.aws_ami.ubuntu: Read complete after 0s [id=ami-027a754129abb5386] aws_subnet.automate-iac-subnet: Refreshing state... [id=subnet-0dbfc00f69e201fa7] aws_network_interface.automate-iac-aws_network_interface: Refreshing state... [id=eni-0673ed65d965c194e] aws_instance.new_instance: Refreshing state... [id=i-08a28371557209b0b] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # aws_instance.new_instance will be destroyed - resource \u0026#34;aws_instance\u0026#34; \u0026#34;new_instance\u0026#34; { - ami = \u0026#34;ami-027a754129abb5386\u0026#34; -\u0026gt; null - arn = \u0026#34;arn:aws:ec2:us-east-1:566027688242:instance/i-08a28371557209b0b\u0026#34; -\u0026gt; null - associate_public_ip_address = false -\u0026gt; null - availability_zone = \u0026#34;us-east-1a\u0026#34; -\u0026gt; null - cpu_core_count = 1 -\u0026gt; null - cpu_threads_per_core = 2 -\u0026gt; null - disable_api_stop = false -\u0026gt; null - disable_api_termination = false -\u0026gt; null - ebs_optimized = false -\u0026gt; null - get_password_data = false -\u0026gt; null - hibernation = false -\u0026gt; null - id = \u0026#34;i-08a28371557209b0b\u0026#34; -\u0026gt; null - instance_initiated_shutdown_behavior = \u0026#34;stop\u0026#34; -\u0026gt; null - instance_state = \u0026#34;running\u0026#34; -\u0026gt; null - instance_type = \u0026#34;t3.micro\u0026#34; -\u0026gt; null - ipv6_address_count = 0 -\u0026gt; null - ipv6_addresses = [] -\u0026gt; null - key_name = \u0026#34;automate-iac-kp\u0026#34; -\u0026gt; null - monitoring = false -\u0026gt; null - placement_partition_number = 0 -\u0026gt; null - primary_network_interface_id = \u0026#34;eni-0673ed65d965c194e\u0026#34; -\u0026gt; null - private_dns = \u0026#34;ip-172-16-10-100.ec2.internal\u0026#34; -\u0026gt; null - private_ip = \u0026#34;172.16.10.100\u0026#34; -\u0026gt; null - secondary_private_ips = [] -\u0026gt; null - security_groups = [] -\u0026gt; null - source_dest_check = true -\u0026gt; null - subnet_id = \u0026#34;subnet-0dbfc00f69e201fa7\u0026#34; -\u0026gt; null - tags = { - \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } -\u0026gt; null - tags_all = { - \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } -\u0026gt; null - tenancy = \u0026#34;default\u0026#34; -\u0026gt; null - user_data_replace_on_change = false -\u0026gt; null - vpc_security_group_ids = [ - \u0026#34;sg-0ce04508e0520a5f7\u0026#34;, ] -\u0026gt; null - capacity_reservation_specification { - capacity_reservation_preference = \u0026#34;open\u0026#34; -\u0026gt; null } - cpu_options { - core_count = 1 -\u0026gt; null - threads_per_core = 2 -\u0026gt; null } - credit_specification { - cpu_credits = \u0026#34;unlimited\u0026#34; -\u0026gt; null } - enclave_options { - enabled = false -\u0026gt; null } - maintenance_options { - auto_recovery = \u0026#34;default\u0026#34; -\u0026gt; null } - metadata_options { - http_endpoint = \u0026#34;enabled\u0026#34; -\u0026gt; null - http_protocol_ipv6 = \u0026#34;disabled\u0026#34; -\u0026gt; null - http_put_response_hop_limit = 1 -\u0026gt; null - http_tokens = \u0026#34;optional\u0026#34; -\u0026gt; null - instance_metadata_tags = \u0026#34;disabled\u0026#34; -\u0026gt; null } - network_interface { - delete_on_termination = false -\u0026gt; null - device_index = 0 -\u0026gt; null - network_card_index = 0 -\u0026gt; null - network_interface_id = \u0026#34;eni-0673ed65d965c194e\u0026#34; -\u0026gt; null } - private_dns_name_options { - enable_resource_name_dns_a_record = false -\u0026gt; null - enable_resource_name_dns_aaaa_record = false -\u0026gt; null - hostname_type = \u0026#34;ip-name\u0026#34; -\u0026gt; null } - root_block_device { - delete_on_termination = true -\u0026gt; null - device_name = \u0026#34;/dev/sda1\u0026#34; -\u0026gt; null - encrypted = false -\u0026gt; null - iops = 100 -\u0026gt; null - tags = {} -\u0026gt; null - throughput = 0 -\u0026gt; null - volume_id = \u0026#34;vol-0a8b14289eb677afa\u0026#34; -\u0026gt; null - volume_size = 8 -\u0026gt; null - volume_type = \u0026#34;gp2\u0026#34; -\u0026gt; null } } # aws_key_pair.new_keypair will be destroyed - resource \u0026#34;aws_key_pair\u0026#34; \u0026#34;new_keypair\u0026#34; { - arn = \u0026#34;arn:aws:ec2:us-east-1:566027688242:key-pair/automate-iac-kp\u0026#34; -\u0026gt; null - fingerprint = \u0026#34;6f:9d:e0:9f:e1:05:17:47:8c:e0:65:ca:dd:6b:7b:1e\u0026#34; -\u0026gt; null - id = \u0026#34;automate-iac-kp\u0026#34; -\u0026gt; null - key_name = \u0026#34;automate-iac-kp\u0026#34; -\u0026gt; null - key_pair_id = \u0026#34;key-0114ec655b0b831e7\u0026#34; -\u0026gt; null - key_type = \u0026#34;rsa\u0026#34; -\u0026gt; null - public_key = \u0026#34;MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQCJgKe/Ko/3pXldymY2/bwP70xPJzP6NpKS9iQP450sWFtc5VuG3fqaOvGMzhrS3VUseALddYALiWmgf+cgjdvASeW3eIl54y2253QTme4eb1WOZ2/MvcANBuiJFqItL7SR4xlaAuI48yS8lh7L2OIIpzy8PUV+EcPD9KGKsAicFwIDAQAB\u0026#34; -\u0026gt; null - tags = { - \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } -\u0026gt; null - tags_all = { - \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } -\u0026gt; null } # aws_network_interface.automate-iac-aws_network_interface will be destroyed - resource \u0026#34;aws_network_interface\u0026#34; \u0026#34;automate-iac-aws_network_interface\u0026#34; { - arn = \u0026#34;arn:aws:ec2:us-east-1:566027688242:network-interface/eni-0673ed65d965c194e\u0026#34; -\u0026gt; null - id = \u0026#34;eni-0673ed65d965c194e\u0026#34; -\u0026gt; null - interface_type = \u0026#34;interface\u0026#34; -\u0026gt; null - ipv4_prefix_count = 0 -\u0026gt; null - ipv4_prefixes = [] -\u0026gt; null - ipv6_address_count = 0 -\u0026gt; null - ipv6_address_list = [] -\u0026gt; null - ipv6_address_list_enabled = false -\u0026gt; null - ipv6_addresses = [] -\u0026gt; null - ipv6_prefix_count = 0 -\u0026gt; null - ipv6_prefixes = [] -\u0026gt; null - mac_address = \u0026#34;02:27:2b:64:3d:c3\u0026#34; -\u0026gt; null - owner_id = \u0026#34;566027688242\u0026#34; -\u0026gt; null - private_ip = \u0026#34;172.16.10.100\u0026#34; -\u0026gt; null - private_ip_list = [ - \u0026#34;172.16.10.100\u0026#34;, ] -\u0026gt; null - private_ip_list_enabled = false -\u0026gt; null - private_ips = [ - \u0026#34;172.16.10.100\u0026#34;, ] -\u0026gt; null - private_ips_count = 0 -\u0026gt; null - security_groups = [ - \u0026#34;sg-0ce04508e0520a5f7\u0026#34;, ] -\u0026gt; null - source_dest_check = true -\u0026gt; null - subnet_id = \u0026#34;subnet-0dbfc00f69e201fa7\u0026#34; -\u0026gt; null - tags = { - \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } -\u0026gt; null - tags_all = { - \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } -\u0026gt; null - attachment { - attachment_id = \u0026#34;eni-attach-05750a0a2ee7869c5\u0026#34; -\u0026gt; null - device_index = 0 -\u0026gt; null - instance = \u0026#34;i-08a28371557209b0b\u0026#34; -\u0026gt; null } } # aws_s3_bucket.new_bucket will be destroyed - resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;new_bucket\u0026#34; { - arn = \u0026#34;arn:aws:s3:::automate-iac-bucket-123123123\u0026#34; -\u0026gt; null - bucket = \u0026#34;automate-iac-bucket-123123123\u0026#34; -\u0026gt; null - bucket_domain_name = \u0026#34;automate-iac-bucket-123123123.s3.amazonaws.com\u0026#34; -\u0026gt; null - bucket_regional_domain_name = \u0026#34;automate-iac-bucket-123123123.s3.us-east-1.amazonaws.com\u0026#34; -\u0026gt; null - force_destroy = false -\u0026gt; null - hosted_zone_id = \u0026#34;Z3AQBSTGFYJSTF\u0026#34; -\u0026gt; null - id = \u0026#34;automate-iac-bucket-123123123\u0026#34; -\u0026gt; null - object_lock_enabled = false -\u0026gt; null - region = \u0026#34;us-east-1\u0026#34; -\u0026gt; null - request_payer = \u0026#34;BucketOwner\u0026#34; -\u0026gt; null - tags = {} -\u0026gt; null - tags_all = {} -\u0026gt; null - grant { - id = \u0026#34;479f57b44641303e6c9a544836dc9b6e50e73a3e558bcb6a2a08f13a3ec6b547\u0026#34; -\u0026gt; null - permissions = [ - \u0026#34;FULL_CONTROL\u0026#34;, ] -\u0026gt; null - type = \u0026#34;CanonicalUser\u0026#34; -\u0026gt; null } - server_side_encryption_configuration { - rule { - bucket_key_enabled = false -\u0026gt; null - apply_server_side_encryption_by_default { - sse_algorithm = \u0026#34;AES256\u0026#34; -\u0026gt; null } } } - versioning { - enabled = false -\u0026gt; null - mfa_delete = false -\u0026gt; null } } # aws_subnet.automate-iac-subnet will be destroyed - resource \u0026#34;aws_subnet\u0026#34; \u0026#34;automate-iac-subnet\u0026#34; { - arn = \u0026#34;arn:aws:ec2:us-east-1:566027688242:subnet/subnet-0dbfc00f69e201fa7\u0026#34; -\u0026gt; null - assign_ipv6_address_on_creation = false -\u0026gt; null - availability_zone = \u0026#34;us-east-1a\u0026#34; -\u0026gt; null - availability_zone_id = \u0026#34;use1-az1\u0026#34; -\u0026gt; null - cidr_block = \u0026#34;172.16.10.0/24\u0026#34; -\u0026gt; null - enable_dns64 = false -\u0026gt; null - enable_lni_at_device_index = 0 -\u0026gt; null - enable_resource_name_dns_a_record_on_launch = false -\u0026gt; null - enable_resource_name_dns_aaaa_record_on_launch = false -\u0026gt; null - id = \u0026#34;subnet-0dbfc00f69e201fa7\u0026#34; -\u0026gt; null - ipv6_native = false -\u0026gt; null - map_customer_owned_ip_on_launch = false -\u0026gt; null - map_public_ip_on_launch = false -\u0026gt; null - owner_id = \u0026#34;566027688242\u0026#34; -\u0026gt; null - private_dns_hostname_type_on_launch = \u0026#34;ip-name\u0026#34; -\u0026gt; null - tags = { - \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } -\u0026gt; null - tags_all = { - \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } -\u0026gt; null - vpc_id = \u0026#34;vpc-02606838ac1e89cea\u0026#34; -\u0026gt; null } # aws_vpc.automate-iac-vpc will be destroyed - resource \u0026#34;aws_vpc\u0026#34; \u0026#34;automate-iac-vpc\u0026#34; { - arn = \u0026#34;arn:aws:ec2:us-east-1:566027688242:vpc/vpc-02606838ac1e89cea\u0026#34; -\u0026gt; null - assign_generated_ipv6_cidr_block = false -\u0026gt; null - cidr_block = \u0026#34;172.16.0.0/16\u0026#34; -\u0026gt; null - default_network_acl_id = \u0026#34;acl-063165fef63bf9e2c\u0026#34; -\u0026gt; null - default_route_table_id = \u0026#34;rtb-087a43ef011a55a02\u0026#34; -\u0026gt; null - default_security_group_id = \u0026#34;sg-0ce04508e0520a5f7\u0026#34; -\u0026gt; null - dhcp_options_id = \u0026#34;dopt-6cefc816\u0026#34; -\u0026gt; null - enable_dns_hostnames = false -\u0026gt; null - enable_dns_support = true -\u0026gt; null - enable_network_address_usage_metrics = false -\u0026gt; null - id = \u0026#34;vpc-02606838ac1e89cea\u0026#34; -\u0026gt; null - instance_tenancy = \u0026#34;default\u0026#34; -\u0026gt; null - ipv6_netmask_length = 0 -\u0026gt; null - main_route_table_id = \u0026#34;rtb-087a43ef011a55a02\u0026#34; -\u0026gt; null - owner_id = \u0026#34;566027688242\u0026#34; -\u0026gt; null - tags = { - \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } -\u0026gt; null - tags_all = { - \u0026#34;Name\u0026#34; = \u0026#34;automate-iac\u0026#34; } -\u0026gt; null } Plan: 0 to add, 0 to change, 6 to destroy. aws_key_pair.new_keypair: Destroying... [id=automate-iac-kp] aws_s3_bucket.new_bucket: Destroying... [id=automate-iac-bucket-123123123] aws_instance.new_instance: Destroying... [id=i-08a28371557209b0b] aws_key_pair.new_keypair: Destruction complete after 0s aws_s3_bucket.new_bucket: Destruction complete after 0s aws_instance.new_instance: Still destroying... [id=i-08a28371557209b0b, 10s elapsed] aws_instance.new_instance: Still destroying... [id=i-08a28371557209b0b, 20s elapsed] aws_instance.new_instance: Still destroying... [id=i-08a28371557209b0b, 30s elapsed] aws_instance.new_instance: Still destroying... [id=i-08a28371557209b0b, 40s elapsed] aws_instance.new_instance: Still destroying... [id=i-08a28371557209b0b, 50s elapsed] aws_instance.new_instance: Still destroying... [id=i-08a28371557209b0b, 1m0s elapsed] aws_instance.new_instance: Still destroying... [id=i-08a28371557209b0b, 1m10s elapsed] aws_instance.new_instance: Destruction complete after 1m10s aws_network_interface.automate-iac-aws_network_interface: Destroying... [id=eni-0673ed65d965c194e] aws_network_interface.automate-iac-aws_network_interface: Destruction complete after 1s aws_subnet.automate-iac-subnet: Destroying... [id=subnet-0dbfc00f69e201fa7] aws_subnet.automate-iac-subnet: Destruction complete after 0s aws_vpc.automate-iac-vpc: Destroying... [id=vpc-02606838ac1e89cea] aws_vpc.automate-iac-vpc: Destruction complete after 0s Destroy complete! Resources: 6 destroyed. ","permalink":"http://localhost:1313/posts/004-automate-iac/","summary":"1. Source Code https://github.com/nhatvo1502/Automate-IaC/tree/main 2. What the code does 2.1. create-resource.yml configure aws credential verify aws credential verify aws s3 permission create s3 bucket for terraform backend copy file from GH to runner verify terraform file terraform init -\u0026gt; apply 2.2 destroy-resource.yml configure aws credential copy file from GH to runner verify terraform file terraform init -\u0026gt; destroy empty s3 backend bucket delete-bucket 2.3 main.tf configure s3 as backend create an s3 bucket create a keypair using a public key from GitHub Secret AMI lookup create VPC create a subnet create a network interface create EC2 instance 3.","title":"build AWS network infrastructure and deploy resources using Terraform"},{"content":"1. Verify sender\u0026rsquo;s email address The sender\u0026rsquo;s origin contains 2 parts: The display name and email@domain.com.\nNhat Vo \u0026lt;nhat.vo@awakenservices.net\u0026gt; Display name: Nhat Vo Email address: nhat.vo@awakenservices.net The display name can be forged while email address cannot.\nThis is an example of phishing emails: Nhat Vo \u0026lt;nhat.vo@123456.com\u0026gt; Display name: Nhat Vo Email address: nhat.vo@12345.com 2. Don\u0026rsquo;t Trust Unsolicited Emails Be cautious of unexpected emails, especially those requesting personal information or urging you to click on links. Verify the legitimacy of the email by contacting the sender through a trusted method.\n3. Look for Spelling and Grammar Errors Phishing emails often contain spelling and grammar mistakes. Legitimate organizations usually have professional communication.\n4. Avoid Clicking on Suspicious Links Hover over links to preview the actual URL before clicking. If the link address looks suspicious or doesn\u0026rsquo;t match the expected destination, avoid clicking.\n5. Verify Requests for Sensitive Information Legitimate organizations will not ask for sensitive information like passwords or credit card details via email. If in doubt, contact the organization directly using official contact information.\n","permalink":"http://localhost:1313/posts/003-phishing-email/","summary":"1. Verify sender\u0026rsquo;s email address The sender\u0026rsquo;s origin contains 2 parts: The display name and email@domain.com.\nNhat Vo \u0026lt;nhat.vo@awakenservices.net\u0026gt; Display name: Nhat Vo Email address: nhat.vo@awakenservices.net The display name can be forged while email address cannot.\nThis is an example of phishing emails: Nhat Vo \u0026lt;nhat.vo@123456.com\u0026gt; Display name: Nhat Vo Email address: nhat.vo@12345.com 2. Don\u0026rsquo;t Trust Unsolicited Emails Be cautious of unexpected emails, especially those requesting personal information or urging you to click on links.","title":"Avoid Phishing and Malicious Emails in 2024"},{"content":"I. Introduction This project is inspired by one of my dear friend. Instead of using the AWS Console to generate a credential report, we would set up a runner on GitHub Actions to automate this task. It\u0026rsquo;s a very cool experiment, and I hope that by the end, I can apply and scale this concept into a part of a bigger project down the road.\nII. Our Game-plan We will leverage Github Action to execute a set of commands to generate AWS Credential Report, download it, decode, pipe the content into csv file and finally copy to a existing S3 bucket.\nTools that we need for this project:\nAWS CLI https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html GitHub CLI https://cli.github.com/ III. Create an AWS CLI user + proper permission Steps:\nCreate custom IAM policy on AWS Console Create CLI user Attach the custom IAM Policy to CLI user Create a CLI Access Key Confiure AWS Credential on local terminal What permission do you need?\n- Generate Credential Report iam:GenerateCredentialReport\n- Download Credential Report iam:GetCredentialReport\n- Write to S3 S3:PutObject\n- List S3 bucket S3:GetPbject\n1. Create custom IAM policy on AWS Console:\nAWS Console \u0026gt; IAM \u0026gt; Under Access management \u0026gt; Policies Copy and paste the following JSON policy to the policy editor. You can also create a seperate policy for S3:PutObject where it target one of your bucket.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:GenerateCredentialReport\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;iam:GetCredentialReport\u0026#34;, \u0026#34;s3:GetObject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } 2. Create CLI user Create a CLI user via AWS Console \u0026gt; IAM \u0026gt; Under Access management \u0026gt; Users \u0026gt; Create user 3. Attach the custom IAM Policy to CLI user Select CLI User \u0026gt; Under Permissions \u0026gt; Permissions policies \u0026gt; Add permission \u0026gt; Select our custom policies 4. Create a CLI Access Key Select CLI User in AWS Console \u0026gt; Security credentials Under Access keys \u0026gt; Create access key \u0026gt; Select Command Line Interface (CLI) \u0026gt; tick the box Confirmation \u0026gt; Next \u0026gt; Create access key\nNote down the Access key and Secret access key, you will need these later 5. Confiure AWS Credential on local terminal\nGo to local terminal or Ctrl + Shift +` in vscode to open terminal (my default terminal is PowerShell) \u0026gt; enter this command\naws configure Copy and paste your CLI user access key\nCopy and paste your CLI user secret access key\nEnter default region (mine is us-west-1)\nEnter default format (mine is json)\nVerify your input by entering this command\naws configure list IV. GitHub Steps:\nCreate Github account from their website if you don\u0026rsquo;t have one ready Create a repository Create secret environment Create AWS access key and secret access key secrets 1. Create Github account from their website if you don\u0026rsquo;t have one ready\nGo to their website and sign-up\nhttps://github.com/ 2. Create a repository\nYou can create a new repo from Github web console or using CLI: Login\ngh auth login Create new repo\ngh repo create If you create repo from web, make sure to clone it to your local\ngh repo clone 3. Create Github Action AWS Secrets\nGo to github.com \u0026gt; Your project repository \u0026gt; Settings \u0026gt; under Security \u0026gt; Secrets and variables \u0026gt; Actions\nNew Environment \u0026gt; Create environment name Dev \u0026gt; Click on Dev\nAdd secret \u0026gt; Name it AWS_ACCESS_KEY_ID \u0026gt; Copy and paste your CLI Access Key from the note\nAdd secret \u0026gt; Name it AWS_SECRET_ACCESS_KEY \u0026gt; Copy and paste your CLI Secret Access Key from the note\nV. VSCODE extension** I use VSCode along with these extensions GitHub Action will support syntax for yml file.\nVI. Github Action Steps:\nCreate .github/workflows Create yaml files Configure yaml files Code explains Commit to Github Test 1. Create .github/workflows\nGo to the repo folder and create this folder structure\nmkdir .github cd .github mkdir workflows Create action.yml\nIt should look like this\n3. Configure yaml files\nOpen action.yml using vscode\nCopy and paste this code block\nname: Automate Credential Report on: # trigger manually workflow_dispatch: jobs: generate-export-credential-report: runs-on: ubuntu-latest environment: Dev steps: - name: Verify aws run: aws --version - name: Configure AWS Credentials env: AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }} AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }} AWS_DEFAULT_REGION: \u0026#39;us-east-1\u0026#39; run: | mkdir -p ~/.aws echo \u0026#34;[default]\u0026#34; \u0026gt; ~/.aws/credentials echo \u0026#34;aws_access_key_id=${AWS_ACCESS_KEY_ID}\u0026#34; \u0026gt;\u0026gt; ~/.aws/credentials echo \u0026#34;aws_secret_access_key=${AWS_SECRET_ACCESS_KEY}\u0026#34; \u0026gt;\u0026gt; ~/.aws/credentials echo \u0026#34;region=${AWS_DEFAULT_REGION}\u0026#34; \u0026gt;\u0026gt; ~/.aws/credentials - name: Verify AWS Credential run: aws configure list - name: Verify s3 permission run: aws s3 ls - name: Generate Credential Report run: aws iam generate-credential-report - name: Wait for 5 second run: sleep 5 - name: Retrieve report run: report_content=$(aws iam get-credential-report --query \u0026#39;Content\u0026#39; --output text) - name: Decode report run: report_text=$(echo \u0026#34;$report_content\u0026#34; | base64 -d) - name: Save report to csv file run: echo \u0026#34;$report_text\u0026#34; \u0026gt; credential-report.csv - name: Copy the report to s3 run: aws s3 cp ./credential-report.csv s3://keypair-1234 4. Code explains\nWe will start with giving this action a name name: Automate Credential Report The on code block will determine the trigger condition, in this case, work_dispatch mean manual trigger. Read more on on: https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions on: workflow_dispatch The jobs structure should be jobs: build: runs-on: *The machine that you want the jobs to be ran on, we call it runner* environment: *Environment to store our secret on git hub* steps: - name: *Name of each step run: *bash command* Using this understanding, it will translate our code above into jobs: generate-export-credential-report: *our build name* runs-on: ubuntu-latest *running on abuntu with latest update* environment: Dev *Our Github secret environment Dev* All ubuntu-latest on github action pre-installed with AWS CLI, we just want to verify if this is true by verify aws version - name: Verify aws run: aws --version Login AWS CLI user to this runner using AWS Access Key and Secret Access Key that we have saved into our note above. Instead of writing our secret key to the yml, we will be passing it from our Github secret environment which I will be showing in a bit. - name: Configure AWS Credentials env: AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }} AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }} AWS_DEFAULT_REGION: \u0026#39;us-east-1\u0026#39; run: | mkdir -p ~/.aws echo \u0026#34;[default]\u0026#34; \u0026gt; ~/.aws/credentials echo \u0026#34;aws_access_key_id=${AWS_ACCESS_KEY_ID}\u0026#34; \u0026gt;\u0026gt; ~/.aws/credentials echo \u0026#34;aws_secret_access_key=${AWS_SECRET_ACCESS_KEY}\u0026#34; \u0026gt;\u0026gt; ~/.aws/credentials echo \u0026#34;region=${AWS_DEFAULT_REGION}\u0026#34; \u0026gt;\u0026gt; ~/.aws/credentials Verify our credential - name: Verify AWS Credential run: aws configure list This step is optional since I only include S3:PutObject in the policy. To be able to verify if we have permision to s3, we also need S3:GetObject. - name: Verify s3 permission run: aws s3 ls Generate AWS Credential Report - name: Generate Credential Report run: aws iam generate-credential-report Sleep for 5 to make sure the AWS finish generate the report before we can download, if we try to download right after generate, there is a chance the report has not yet been created therefore the script will timeout - name: Wait for 5 second run: sleep 5 Download the report into a variable report_content - name: Retrieve report run: report_content=$(aws iam get-credential-report --query \u0026#39;Content\u0026#39; --output text) Since the content we just downloaded is encrypted Base64, we need to decode it - name: Decode report run: report_text=$(echo \u0026#34;$report_content\u0026#34; | base64 -d) Save it as csv file - name: Save report to csv file run: echo \u0026#34;$report_text\u0026#34; \u0026gt; credential-report.csv Then copy it to S3 - name: Copy the report to s3 run: aws s3 cp ./credential-report.csv s3://keypair-1234 5. Commit to Github\nEnter this command to commit the code to github\ngit add *.* git commit -m \u0026#39;my first commit\u0026#39; git push Go to github.com \u0026gt; your repo and check if your code is there\nGo to Actions tab \u0026gt; if the folder structure .github/workflows/action.yml was done right, you should see this Drop down Run workflow button \u0026gt; Run workflow \u0026gt; Click onto the workflow and see all the steps are executing in real time If all the steps are succesfully executed \u0026gt; go to AWS Console \u0026gt; S3 \u0026gt; your bucket \u0026gt; you should see your report Download it and open with excel to see if the decoding works\n","permalink":"http://localhost:1313/posts/002-aws-credential-report/","summary":"I. Introduction This project is inspired by one of my dear friend. Instead of using the AWS Console to generate a credential report, we would set up a runner on GitHub Actions to automate this task. It\u0026rsquo;s a very cool experiment, and I hope that by the end, I can apply and scale this concept into a part of a bigger project down the road.\nII. Our Game-plan We will leverage Github Action to execute a set of commands to generate AWS Credential Report, download it, decode, pipe the content into csv file and finally copy to a existing S3 bucket.","title":"get aws credential report using github action"},{"content":"\u0026ldquo;Humility is not thinking less of yourself, it\u0026rsquo;s thinking of yourself less.\u0026rdquo;\nRick Warren ","permalink":"http://localhost:1313/posts/about-me/","summary":"\u0026ldquo;Humility is not thinking less of yourself, it\u0026rsquo;s thinking of yourself less.\u0026rdquo;\nRick Warren ","title":"About Me"}]